{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN Texto Animales.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9Uk_DNw5983j","colab_type":"text"},"source":["# Generación de Texto de \"Origen de las Especies\" usando una  Recurrent Neuronal Network del tipo RNN básico, LSTM o GRU\n","Basado en https://www.tensorflow.org/tutorials/text/text_generation"]},{"cell_type":"markdown","metadata":{"id":"7Mjqn0ta-JmL","colab_type":"text"},"source":["1) Cargar las librerías:"]},{"cell_type":"code","metadata":{"id":"MAgWSjOw-JwI","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Librerías a usar\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","\n","import numpy as np\n","import os\n","import time\n","\n","from google.colab import files \n","import io\n","\n","print(\"Librerías cargadas\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGFu1mdg-N5w","colab_type":"text"},"source":["2) Cargar el texto base a procesar:"]},{"cell_type":"code","metadata":{"id":"etDVW375lv5u","colab_type":"code","colab":{},"cellView":"form"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# directorio local en Google Drive\n","path = 'gdrive/My Drive/IA/demo ANIMALES/datos'  #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1h1PnwK-Q1Y","colab_type":"code","colab":{},"cellView":"form"},"source":["nombre_archivo = \"/La_Evolucion_De_Las_Especies.txt\"  #@param {type:\"string\"}\n","\n","# levanta el archivo de texto del Drive para procesar\n","text = open(\"\".join([path, nombre_archivo]), 'rb').read().decode(encoding='utf-8')\n","\n","print(\"> Archivo cargado:\")\n","print (' -- Tamaño total del texto: {} caracteres'.format(len(text)))\n","\n","# muestra los primeros 250 caracteres del texto\n","print(\"\\n -- Ejemplo: \\n\", text[:250])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mIgEntMT-ZbB","colab_type":"text"},"source":["3) Preparar el texto base a procesar:"]},{"cell_type":"code","metadata":{"id":"W0NuYR750LvF","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Limpiar el texto\n","# eliminar saltos de línea y signos de puntuación especiales\n","text = text.replace('\\n', ' ')\n","text = text.replace('\\t', ' ')\n","text = text.replace('-', ' ')\n","text = text.replace(':', ' ')\n","text = text.replace(',', ' ')\n","text = text.replace(';', ' ')\n","text = text.replace('.', ' ')\n","text = text.replace('\\'', ' ')\n","text = text.replace('\"', ' ')\n","text = text.replace('`', ' ')\n","text = text.replace('(', ' ')\n","text = text.replace(')', ' ')\n","text = text.replace('!', ' ')\n","text = text.replace('?', ' ')\n","text = text.replace('<', ' ')\n","text = text.replace('>', ' ')\n","text = text.replace('=', ' ')\n","text = text.replace('/', ' ')\n","text = text.replace('@', ' ')\n","text = text.replace('_', ' ')\n","text = text.replace('  ', ' ')\n","\n","# pasa todo a minúsculas\n","text = text.lower()\n","\n","# eliminar acentos (reemplaza por letra sin acento)\n","text = text.replace('á', 'a')\n","text = text.replace('é', 'e')\n","text = text.replace('í', 'i')\n","text = text.replace('ó', 'o')\n","text = text.replace('ú', 'u')\n","\n","# muestra los primeros 250 caracteres del texto\n","print(\"\\n -- Ejemplo luego de preparar: \\n\", text[:250])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jvaOGfdL-gnP","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Preparar texto para RNN\n","\n","# The unique characters in the file\n","vocab = sorted(set(text))\n","print ('{} caracteres distintos detectados'.format(len(vocab)))\n","\n","# Creating a mapping from unique characters to indices\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","text_as_int = np.array([char2idx[c] for c in text])\n","\n","print('{')\n","for char,_ in zip(char2idx, range(20)):\n","    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n","print('  ...\\n}')\n","\n","# Muestra ejemplo de cómo se mapean los caracteres a valores numéricos\n","print ('{} <-------- > {}'.format(repr(text[:13]), text_as_int[:13]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jph0s63Q-s55","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Armar secuencias de texto y formatear\n","\n","# determinar el largo máximo de la secuencia\n","if ((len(text)//101)<1000):\n","  seq_length = 50\n","else:\n","  seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1)\n","print(\"Largo de secuencias: \", seq_length)\n","print(\"\\n\")\n","print(\"Ejemplos por época: \", examples_per_epoch)\n","\n","# Dividir en datos de entrenamiento y prueba, para ello divide el texto en secuencias donde \n","#- la secuencia de la posición 0 a [seq_length] se considera de entrada, y \n","#- la secuencia de la posición  [seq_length+1] al final es la de salida\n","\n","# genera un vector de caracteres\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","# procesa para generar las secuencias el largo deseado\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","# muestra ejemplo\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))\n","\n","\n","# genera las secuencias de entrada y salida\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","datasetSeq = sequences.map(split_input_target)\n","\n","print(\"DatasetSeq: \", datasetSeq, \"\\n\")\n","\n","# muestra ejemplo\n","for input_example, target_example in  datasetSeq.take(2):\n","  print ('Texto de Entrada: ', repr(''.join(idx2char[input_example.numpy()])))\n","  print ('Texto  de Salida:', repr(''.join(idx2char[target_example.numpy()])))\n","  print(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yyj9bjy8-2Qp","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Ejemplos\n","\n","# muestra entrada y salida por cada caracter\n","for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","    print(\"Step {:4d}\".format(i))\n","    print(\"  Entrada: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n","    print(\"  Salida Esperada: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jpktfg15-48J","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Armar repositorio de datos de entrenamiento\n","# genera 'batch' de secuencias que se van a procesar en el entrenamiento\n","\n","# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 100000\n","\n","dataset = datasetSeq.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","print(\"Dataset: \", dataset, \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Opx54HBy-8Cw","colab_type":"text"},"source":["4) Especificar y preparar el modelo de la RNN a usar:"]},{"cell_type":"code","metadata":{"id":"SbuPSVfReQrT","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Establecer modelo\n","\n","# Seleccione el modelo a usar\n","seleccModel = 'LSTM'  #@param [\"LSTM\", \"GRU\", \"RNN\"]\n","\n","# cantidad de neuronas RNN\n","rnn_units = 1024 #@param {type:\"integer\"}\n","\n","# define el modelo a utilizar\n","if seleccModel == 'LSTM': \n","    # define el modelo LSTM\n","    def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","      model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                  batch_input_shape=[batch_size, None]),\n","        tf.keras.layers.LSTM(rnn_units,\n","                            return_sequences=True,\n","                            stateful=True,\n","                            recurrent_initializer='glorot_uniform'),\n","        tf.keras.layers.Dense(vocab_size)\n","      ])\n","      return model\n","\n","    print(\"Modelo LSTM definido.\")\n","\n","elif seleccModel == 'GRU': \n","    # define el modelo GRU\n","    def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","      model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                  batch_input_shape=[batch_size, None]),\n","        tf.keras.layers.GRU(rnn_units,\n","                            return_sequences=True,\n","                            stateful=True,\n","                            recurrent_initializer='glorot_uniform'),\n","        tf.keras.layers.Dense(vocab_size)\n","      ])\n","      return model\n","\n","    print(\"Modelo GRU definido.\")\n","\n","else: #if seleccModel == 'RNN': \n","    # define el modelo RNN básico\n","    def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","      model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                  batch_input_shape=[batch_size, None]),\n","        tf.keras.layers.SimpleRNN(rnn_units,\n","                            return_sequences=True,\n","                            stateful=True,\n","                            recurrent_initializer='glorot_uniform'),\n","        tf.keras.layers.Dense(vocab_size)\n","      ])\n","      return model\n","\n","    print(\"Modelo RNN definido.\")\n","\n","# Length of the vocabulary in chars\n","vocab_size = len(vocab)\n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# genera el modelo\n","model = build_model(\n","  vocab_size = len(vocab),\n","  embedding_dim=embedding_dim,\n","  rnn_units=rnn_units,\n","  batch_size=BATCH_SIZE)\n","\n","model.summary()\n","\n","# prepara variables auxiliares para el entrenamiento  de la RNN\n","for input_example_batch, target_example_batch in dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n","\n","# compila el modelo para el entrenamiento  de la RNN\n","def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n","print(\"Forma vector predicción: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n","\n","model.compile(optimizer='adam', loss=loss)\n","\n","print(\"\\nModelo generado\", model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c2Q1uWbU_XqH","colab_type":"text"},"source":["5) Entrenar la RNN:"]},{"cell_type":"code","metadata":{"id":"LCwiXCwO_jLz","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Entrenar\n","\n","# define donde se a almacenar la información de checkpoints\n","\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './checkpoints/RNN_training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","print(\"Checkpoints grabados en \", checkpoint_dir)\n","print(\"\\n\")\n","\n","# ejecutar el entrenamiento (poner antes en GPU)\n","EPOCHS = 100\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6j_vf8-1_xWM","colab_type":"text"},"source":["6) Probar la RNN entrenada:"]},{"cell_type":"code","metadata":{"id":"S9RSE-cm_5OK","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Cargar modelo entrenado \n","\n","# recupera la información del último checkpoint\n","tf.train.latest_checkpoint(checkpoint_dir)\n","\n","modelPred = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","modelPred.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","modelPred.build(tf.TensorShape([1, None]))\n","\n","modelPred.summary()\n","\n","# define función auxiliar para devolver predicción de texto\n","def generate_text(model, temperature, start_string, num_generate=500):\n","\n","  # Converting our start string to numbers (vectorizing)\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # Empty string to store our results\n","  text_generated = []\n","\n","  # Here batch size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # remove the batch dimension\n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # using a categorical distribution to predict the word returned by the model\n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # We pass the predicted word as the next input to the model\n","      # along with the previous hidden state\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","\n","      text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))\n","\n","\n","# ejecuta el modelo usando como entrada un texto para probar\n","print(\"\\n\\n--------------------------------------------------------------------------------------------\\n\")\n","print(generate_text(modelPred, temperature=1.0, start_string=u\"darwin\"))\n","print(\"\\n--------------------------------------------------------------------------------------------\\n\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlnoBnACoAc9","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Probar generación de texto:\n","\n","# Grado de \"temperatura\" u originalidad que va a generar el algoritmo:\n","# - cuanto más alto el valor, se genera texto \"más sorprendente\".\n","# - cuanto más bajo, se genera texto \"más esperado\".\n","originalidad =  0.9 #@param {type:\"number\"}\n","\n","# Texto inicial para generar\n","texto_inicial = 'mamifero' #@param {type:\"string\" }\n","\n","# Largo del texto a generar\n","largo_texto = 500 #@param {type:\"integer\" }\n","\n","# ejecuta el modeo usando como entrada texto_inicial\n","print(\"\\n\\n--------------------------------------------------------------------------------------------\\n\")\n","print(generate_text(modelPred, temperature=originalidad, start_string=texto_inicial, num_generate=largo_texto))\n","print(\"\\n--------------------------------------------------------------------------------------------\\n\\n\")"],"execution_count":null,"outputs":[]}]}
